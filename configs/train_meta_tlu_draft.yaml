# train_dataset: image-folder
# train_dataset_args:
#     root_path: data/tlu-states/images
#     split_file: data/split.json
#     split: train
#     augment: True

# val_dataset: image-folder
# val_dataset_args:
#     root_path: data/tlu-states/images
#     split_file: data/split.json
#     split: val
#     augment: False

# model: meta-baseline
# model_args: 
#     encoder: convnet4
#     method: cos # or whatever AGNN uses normally, but meta-baseline often implies a specific head

# # Config for AGNN specific (based on train_meta_mini.yaml structure usually)
# # Actually, the user's repo has train_meta.py. I should check configs/train_meta_mini.yaml structure to match it.
# # This file is a placeholder based on assumptions. I will likely need to adjust it to match train_meta.yaml structure properly.
# # For now, let's stick to the classifier config which is the immediate next step.
# # I will make a minimal valid 'meta' config just in case.

# n_way: 5
# n_shot: 1
# n_query: 15
# train_batches: 200
# test_batches: 200 # number of episodes (val)

# ep_per_batch: 1

# batch_size: 1 # ignored by CategoriesSampler usually, or used as n_episodes?
# max_epoch: 100
# optimizer: sgd
# optimizer_args: {lr: 0.001, weight_decay: 5.e-4}

# visualize_datasets: False
train_dataset: image-folder-custom
train_dataset_args:
    root_path: /content/drive/MyDrive/Do_an_Data/tlu-states/images
    split_file: /content/drive/MyDrive/Do_an_Data/split.json
    split: train
    augment: True
    image_size: 84

tval_dataset: image-folder-custom
tval_dataset_args:
    root_path: /content/drive/MyDrive/Do_an_Data/tlu-states/images
    split_file: /content/drive/MyDrive/Do_an_Data/split.json
    split: test
    augment: False
    image_size: 84

val_dataset: image-folder-custom
val_dataset_args:
    root_path: /content/drive/MyDrive/Do_an_Data/tlu-states/images
    split_file: /content/drive/MyDrive/Do_an_Data/split.json
    split: val
    augment: False
    image_size: 84

model: gnn
model_args: 
    encoder: convnet4 # tùy chỉnh theo backbone
    encoder_args: {}
# IMPORTANT: Update this path to point to your trained backbone!

n_way: 5
n_shot: 1
n_query: 15
train_batches: 20
test_batches: 20
ep_per_batch: 2

max_epoch: 100 # Adjusted for smaller dataset/testing first
optimizer: adam
optimizer_args: {lr: 0.001, weight_decay: 1.e-5}

visualize_datasets: False


# train_dataset: image-folder
# train_dataset_args:
#     root_path: data/tlu-states/images
#     split_file: data/split.json
#     split: train
#     augment: True

# val_dataset: image-folder
# val_dataset_args:
#     root_path: data/tlu-states/images
#     split_file: data/split.json
#     split: val
#     augment: False

# model: meta-baseline
# model_args: 
#     encoder: convnet4
#     method: cos # or whatever AGNN uses normally, but meta-baseline often implies a specific head

# # Config for AGNN specific (based on train_meta_mini.yaml structure usually)
# # Actually, the user's repo has train_meta.py. I should check configs/train_meta_mini.yaml structure to match it.
# # This file is a placeholder based on assumptions. I will likely need to adjust it to match train_meta.yaml structure properly.
# # For now, let's stick to the classifier config which is the immediate next step.
# # I will make a minimal valid 'meta' config just in case.

# n_way: 5
# n_shot: 1
# n_query: 15
# train_batches: 200
# test_batches: 200 # number of episodes (val)

# ep_per_batch: 1

# batch_size: 1 # ignored by CategoriesSampler usually, or used as n_episodes?
# max_epoch: 100
# optimizer: sgd
# optimizer_args: {lr: 0.001, weight_decay: 5.e-4}

# visualize_datasets: False
##########################################################################
train_dataset: image-folder-custom
train_dataset_args:
    root_path: /content/drive/MyDrive/Do_an_Data/tlu-states/images
    split_file: /content/drive/MyDrive/Do_an_Data/split.json
    split: train
    augment: True
    image_size: 84

tval_dataset: image-folder-custom
tval_dataset_args:
    root_path: /content/drive/MyDrive/Do_an_Data/tlu-states/images
    split_file: /content/drive/MyDrive/Do_an_Data/split.json
    split: test
    augment: False
    image_size: 84

val_dataset: image-folder-custom
val_dataset_args:
    root_path: /content/drive/MyDrive/Do_an_Data/tlu-states/images
    split_file: /content/drive/MyDrive/Do_an_Data/split.json
    split: val
    augment: False
    image_size: 84

model: gnn
model_args: 
    encoder: resnet12 # tùy chỉnh theo backbone
    encoder_args: {}
# IMPORTANT: Update this path to point to your trained backbone!

n_way: 5
n_shot: 1
n_query: 15
train_batches: 20
test_batches: 20
ep_per_batch: 2

max_epoch: 100 # Adjusted for smaller dataset/testing first
optimizer: adam
optimizer_args: {lr: 0.001, weight_decay: 1.e-5}

visualize_datasets: False
